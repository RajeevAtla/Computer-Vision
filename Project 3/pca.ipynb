{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727d36cb-778d-428b-b2de-13777bd1b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7580c216-ce10-4834-ac72-1373c21cb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for datasets\n",
    "datasets = [\"Dataset1\", \"Dataset2\", \"Dataset3\"]\n",
    "output_dir = \"output_pca\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67123f5f-e706-41c0-ba95-a02344b179fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using PCA\n",
    "def extract_pca_features(image_path, pca_pipeline):\n",
    "    \"\"\"Extract PCA-based features from an image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale for simplicity\n",
    "    image_resized = image.resize((224, 224))  # Resize to fixed dimensions\n",
    "    image_array = np.array(image_resized).flatten()  # Flatten the image to 1D array\n",
    "    return pca_pipeline.transform([image_array])[0]  # Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca554734-55fd-4eeb-8f25-f7d08b548c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_pca(dataset_name):\n",
    "    \"\"\"Process a single dataset: extract features using PCA, train SVM, and save results.\"\"\"\n",
    "    dataset_path = dataset_name\n",
    "    output_path = os.path.join(output_dir, f\"{dataset_name}_features.npy\")\n",
    "\n",
    "    # Check if features already exist\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"Extracting PCA features for {dataset_name}...\")\n",
    "        features = []\n",
    "        labels = []\n",
    "        class_to_idx = {cls_name: idx for idx, cls_name in enumerate(os.listdir(dataset_path))}\n",
    "\n",
    "        # Prepare data for PCA training\n",
    "        images = []\n",
    "        for class_name, class_idx in class_to_idx.items():\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                try:\n",
    "                    image = Image.open(img_path).convert(\"L\").resize((224, 224))\n",
    "                    images.append(np.array(image).flatten())\n",
    "                    labels.append(class_idx)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Determine the maximum allowable n_components for PCA\n",
    "        max_components = min(len(images), images.shape[1])\n",
    "        print(f\"Setting PCA n_components to {max_components} for {dataset_name}\")\n",
    "\n",
    "        # Train PCA on the dataset\n",
    "        scaler = StandardScaler()\n",
    "        pca = PCA(n_components=max_components)  # Dynamically adjust components\n",
    "        pca_pipeline = Pipeline([('scaler', scaler), ('pca', pca)])\n",
    "        pca_pipeline.fit(images)\n",
    "\n",
    "        # Extract PCA features\n",
    "        features = pca_pipeline.transform(images)\n",
    "        np.save(output_path, {\"features\": features, \"labels\": labels, \"pca_pipeline\": pca_pipeline})\n",
    "        print(f\"PCA features saved for {dataset_name} to {output_path}\")\n",
    "    else:\n",
    "        print(f\"PCA features file already exists for {dataset_name}. Loading...\")\n",
    "\n",
    "        # Load features and labels\n",
    "        data = np.load(output_path, allow_pickle=True).item()\n",
    "        features, labels, pca_pipeline = data[\"features\"], data[\"labels\"], data[\"pca_pipeline\"]\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    features = features[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Train an SVM classifier on the entire dataset\n",
    "    svm_clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "    svm_clf.fit(features, labels)\n",
    "\n",
    "    # Predictions using the same data\n",
    "    predictions = svm_clf.predict(features)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    print(f\"Accuracy for {dataset_name}: {accuracy:.2f}\")\n",
    "\n",
    "    # Detailed classification report\n",
    "    print(f\"\\nClassification Report for {dataset_name}:\")\n",
    "    print(classification_report(labels, predictions, target_names=os.listdir(dataset_path)))\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=os.listdir(dataset_path), yticklabels=os.listdir(dataset_path))\n",
    "    plt.title(f\"Confusion Matrix for {dataset_name}\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    confusion_matrix_path = os.path.join(output_dir, f\"{dataset_name}_confusion_matrix.png\")\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Confusion matrix for {dataset_name} saved to {confusion_matrix_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e26f1c8-14f2-4eaf-bf8c-2957d2b2be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting PCA features for Dataset1...\n",
      "Setting PCA n_components to 40 for Dataset1\n",
      "PCA features saved for Dataset1 to output_pca\\Dataset1_features.npy\n",
      "Accuracy for Dataset1: 1.00\n",
      "\n",
      "Classification Report for Dataset1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Laptops       1.00      1.00      1.00        20\n",
      " Smartphones       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Confusion matrix for Dataset1 saved to output_pca\\Dataset1_confusion_matrix.png\n",
      "Extracting PCA features for Dataset2...\n",
      "Setting PCA n_components to 40 for Dataset2\n",
      "PCA features saved for Dataset2 to output_pca\\Dataset2_features.npy\n",
      "Accuracy for Dataset2: 1.00\n",
      "\n",
      "Classification Report for Dataset2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cars       1.00      1.00      1.00        20\n",
      "      Planes       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Confusion matrix for Dataset2 saved to output_pca\\Dataset2_confusion_matrix.png\n",
      "Extracting PCA features for Dataset3...\n",
      "Setting PCA n_components to 40 for Dataset3\n",
      "PCA features saved for Dataset3 to output_pca\\Dataset3_features.npy\n",
      "Accuracy for Dataset3: 1.00\n",
      "\n",
      "Classification Report for Dataset3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Birds       1.00      1.00      1.00        20\n",
      "     Turtles       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Confusion matrix for Dataset3 saved to output_pca\\Dataset3_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# Process all datasets\n",
    "for dataset in datasets:\n",
    "    process_dataset_pca(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
